{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self-Driving Cars.ipynb",
      "provenance": [],
      "mount_file_id": "1t9tpg5C1QrytdR0qycB2nzNwqGRh--E4",
      "authorship_tag": "ABX9TyMAf6iaxV/DpUUA3yBhNT3v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dxv2k/AI_RACING_GAME/blob/Udacity/Self_Driving_Cars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAD68A2rc1mS",
        "colab_type": "text"
      },
      "source": [
        "Download dataset from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvgOL9Ejcuiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0zEltrmc39c",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "403fd0bc-93fe-405b-a663-df6d04f78903"
      },
      "source": [
        "# Upload kaggle API key file\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17b64ac6-971e-4e70-b2a9-2f8d4b616849\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-17b64ac6-971e-4e70-b2a9-2f8d4b616849\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZDU1vgoc6hU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65219b98-66fd-4c72-93bc-852ca668a95c"
      },
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjg9RKR3dC9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0Z-agVidG46",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66ff81f3-a823-4ed0-c44c-20d17933cf41"
      },
      "source": [
        "!kaggle datasets download -d zaynena/selfdriving-car-simulator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "selfdriving-car-simulator.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SHh2x7bdNwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f00a1a15-1dbb-4467-fbb9-822bc84e40d2"
      },
      "source": [
        "! mkdir selfdriving-car-simulator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘selfdriving-car-simulator’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8q4CwmOdib6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "26e6819f-a842-45e8-ea99-9d9e6a1e813e"
      },
      "source": [
        "! unzip selfdriving-car-simulator.zip -d selfdriving-car-simulator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  selfdriving-car-simulator.zip\n",
            "replace selfdriving-car-simulator/dataset/dataset/IMG/Thumbs.db? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-7feb3a0b0c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' unzip selfdriving-car-simulator.zip -d selfdriving-car-simulator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJP-T8_Lifpw",
        "colab_type": "text"
      },
      "source": [
        "Colect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozz3N2o3dpFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import MultiStepLR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8oAhn1fiob1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aik1pQ8qjCq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/selfdriving-car-simulator/dataset/dataset/driving_log.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm87tr5KkAUp",
        "colab_type": "text"
      },
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FjcwwPCjuv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataroot = \"selfdriving-car-simulator/dataset/dataset/\"\n",
        "ckptroot = \"./\"\n",
        "\n",
        "lr = 1e-4\n",
        "weight_decay = 1e-5\n",
        "batch_size = 32\n",
        "num_workers = 8\n",
        "test_size = 0.8\n",
        "shuffle = True\n",
        "\n",
        "epochs = 80\n",
        "start_epoch = 0\n",
        "resume = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNNB1G2ukOO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def toDevice(datas, device):\n",
        "    \"\"\"Enable cuda.\"\"\"\n",
        "    imgs, angles = datas\n",
        "    return imgs.float().to(device), angles.float().to(device)\n",
        "\n",
        "\n",
        "def augment(dataroot, imgName, angle):\n",
        "    \"\"\"Data augmentation.\"\"\"\n",
        "    name = dataroot + 'IMG/' + imgName.split('\\\\')[-1]\n",
        "    current_image = cv2.imread(name)\n",
        "\n",
        "    if current_image is None:\n",
        "        print(name)\n",
        "\n",
        "    current_image = current_image[65:-25, :, :]\n",
        "    if np.random.rand() < 0.5:\n",
        "        current_image = cv2.flip(current_image, 1)\n",
        "        angle = angle * -1.0\n",
        "\n",
        "    return current_image, angle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXYyEaFEkSFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(data_dir, test_size):\n",
        "    \"\"\"Load training data and train validation split\"\"\"\n",
        "    pass\n",
        "\n",
        "    # reads CSV file into a single dataframe variable\n",
        "    data_df = pd.read_csv(os.path.join(data_dir, 'driving_log.csv'),\n",
        "                          names=['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed'])\n",
        "\n",
        "    # Divide the data into training set and validation set\n",
        "    train_len = int(test_size * data_df.shape[0])\n",
        "    valid_len = data_df.shape[0] - train_len\n",
        "    trainset, valset = data.random_split(\n",
        "        data_df.values.tolist(), lengths=[train_len, valid_len])\n",
        "\n",
        "    return trainset, valset\n",
        "\n",
        "trainset, valset = load_data(dataroot, test_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TxAA_FzkUvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TripletDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, dataroot, samples, transform=None):\n",
        "        self.samples = samples\n",
        "        self.dataroot = dataroot\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_samples = self.samples[index]\n",
        "        steering_angle = float(batch_samples[3])\n",
        "\n",
        "        center_img, steering_angle_center = augment(self.dataroot, batch_samples[0], steering_angle)\n",
        "        left_img, steering_angle_left     = augment(self.dataroot, batch_samples[1], steering_angle + 0.4)\n",
        "        right_img, steering_angle_right   = augment(self.dataroot, batch_samples[2], steering_angle - 0.4)\n",
        "\n",
        "        center_img = self.transform(center_img)\n",
        "        left_img   = self.transform(left_img)\n",
        "        right_img  = self.transform(right_img)\n",
        "\n",
        "        return (center_img, steering_angle_center), (left_img, steering_angle_left), (right_img, steering_angle_right)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krbCXBjckg_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_loader(dataroot, trainset, valset, batch_size, shuffle, num_workers):\n",
        "    \"\"\"Self-Driving vehicles simulator dataset Loader.\n",
        "\n",
        "    Args:\n",
        "        trainset: training set\n",
        "        valset: validation set\n",
        "        batch_size: training set input batch size\n",
        "        shuffle: whether shuffle during training process\n",
        "        num_workers: number of workers in DataLoader\n",
        "\n",
        "    Returns:\n",
        "        trainloader (torch.utils.data.DataLoader): DataLoader for training set\n",
        "        testloader (torch.utils.data.DataLoader): DataLoader for validation set\n",
        "    \"\"\"\n",
        "    transformations = transforms.Compose(\n",
        "        [transforms.Lambda(lambda x: (x / 127.5) - 1.0)])\n",
        "\n",
        "    # Load training data and validation data\n",
        "    training_set = TripletDataset(dataroot, trainset, transformations)\n",
        "    trainloader = DataLoader(training_set,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers)\n",
        "\n",
        "    validation_set = TripletDataset(dataroot, valset, transformations)\n",
        "    valloader = DataLoader(validation_set,\n",
        "                           batch_size=batch_size,\n",
        "                           shuffle=shuffle,\n",
        "                           num_workers=num_workers)\n",
        "\n",
        "    return trainloader, valloader\n",
        "\n",
        "\n",
        "trainloader, validationloader = data_loader(dataroot,\n",
        "                                            trainset, valset,\n",
        "                                            batch_size,\n",
        "                                            shuffle,\n",
        "                                            num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMhPvTkOkpFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetworkNvidia(nn.Module):\n",
        "    \"\"\"NVIDIA model used in the paper.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize NVIDIA model.\n",
        "\n",
        "        NVIDIA model used\n",
        "            Image normalization to avoid saturation and make gradients work better.\n",
        "            Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n",
        "            Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n",
        "            Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n",
        "            Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
        "            Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
        "            Drop out (0.5)\n",
        "            Fully connected: neurons: 100, activation: ELU\n",
        "            Fully connected: neurons: 50, activation: ELU\n",
        "            Fully connected: neurons: 10, activation: ELU\n",
        "            Fully connected: neurons: 1 (output)\n",
        "\n",
        "        the convolution layers are meant to handle feature engineering\n",
        "        the fully connected layer for predicting the steering angle.\n",
        "        \"\"\"\n",
        "        super(NetworkNvidia, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 24, 5, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(24, 36, 5, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(36, 48, 5, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(48, 64, 3),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(64, 64, 3),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=64 * 2 * 33, out_features=100),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(in_features=100, out_features=50),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(in_features=50, out_features=10),\n",
        "            nn.Linear(in_features=10, out_features=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        input = input.view(input.size(0), 3, 70, 320)\n",
        "        output = self.conv_layers(input)\n",
        "        # print(output.shape)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.linear_layers(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Define model\n",
        "print(\"==> Initialize model ...\")\n",
        "model = NetworkNvidia()\n",
        "print(\"==> Initialize model done ...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCgcH_qfkvyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=lr,\n",
        "                       weight_decay=weight_decay)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOp6OtNak1bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning rate scheduler\n",
        "scheduler = MultiStepLR(optimizer, milestones=[30, 50], gamma=0.1)\n",
        "\n",
        "# transfer to gpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttwOiiN8k49G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if resume:\n",
        "    print(\"==> Loading checkpoint ...\")\n",
        "    checkpoint = torch.load(\"../input/pretrainedmodels/both-nvidia-model-61.h5\",\n",
        "                            map_location=lambda storage, loc: storage)\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgpcWCf5k7WI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer(object):\n",
        "    \"\"\"Trainer.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 ckptroot,\n",
        "                 model,\n",
        "                 device,\n",
        "                 epochs,\n",
        "                 criterion,\n",
        "                 optimizer,\n",
        "                 scheduler,\n",
        "                 start_epoch,\n",
        "                 trainloader,\n",
        "                 validationloader):\n",
        "        \"\"\"Self-Driving car Trainer.\n",
        "\n",
        "        Args:\n",
        "            model:\n",
        "            device:\n",
        "            epochs:\n",
        "            criterion:\n",
        "            optimizer:\n",
        "            start_epoch:\n",
        "            trainloader:\n",
        "            validationloader:\n",
        "\n",
        "        \"\"\"\n",
        "        super(Trainer, self).__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "        self.ckptroot = ckptroot\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.start_epoch = start_epoch\n",
        "        self.trainloader = trainloader\n",
        "        self.validationloader = validationloader\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Training process.\"\"\"\n",
        "        self.model.to(self.device)\n",
        "        for epoch in range(self.start_epoch, self.epochs + self.start_epoch):\n",
        "            self.scheduler.step()\n",
        "            \n",
        "            # Training\n",
        "            train_loss = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for local_batch, (centers, lefts, rights) in enumerate(self.trainloader):\n",
        "                # Transfer to GPU\n",
        "                centers, lefts, rights = toDevice(centers, self.device), toDevice(\n",
        "                    lefts, self.device), toDevice(rights, self.device)\n",
        "\n",
        "                # Model computations\n",
        "                self.optimizer.zero_grad()\n",
        "                datas = [centers, lefts, rights]\n",
        "                for data in datas:\n",
        "                    imgs, angles = data\n",
        "                    # print(\"training image: \", imgs.shape)\n",
        "                    outputs = self.model(imgs)\n",
        "                    loss = self.criterion(outputs, angles.unsqueeze(1))\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                    train_loss += loss.data.item()\n",
        "\n",
        "                if local_batch % 100 == 0:\n",
        "\n",
        "                    print(\"Training Epoch: {} | Loss: {}\".format(epoch, train_loss / (local_batch + 1)))\n",
        "\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            valid_loss = 0\n",
        "            with torch.set_grad_enabled(False):\n",
        "                for local_batch, (centers, lefts, rights) in enumerate(self.validationloader):\n",
        "                    # Transfer to GPU\n",
        "                    centers, lefts, rights = toDevice(centers, self.device), toDevice(\n",
        "                        lefts, self.device), toDevice(rights, self.device)\n",
        "\n",
        "                    # Model computations\n",
        "                    self.optimizer.zero_grad()\n",
        "                    datas = [centers, lefts, rights]\n",
        "                    for data in datas:\n",
        "                        imgs, angles = data\n",
        "                        outputs = self.model(imgs)\n",
        "                        loss = self.criterion(outputs, angles.unsqueeze(1))\n",
        "\n",
        "                        valid_loss += loss.data.item()\n",
        "\n",
        "                    if local_batch % 100 == 0:\n",
        "                        print(\"Validation Loss: {}\".format(valid_loss / (local_batch + 1)))\n",
        "\n",
        "\n",
        "            print()\n",
        "            # Save model\n",
        "            if epoch % 5 == 0 or epoch == self.epochs + self.start_epoch - 1:\n",
        "\n",
        "                state = {\n",
        "                    'epoch': epoch + 1,\n",
        "                    'state_dict': self.model.state_dict(),\n",
        "                    'optimizer': self.optimizer.state_dict(),\n",
        "                    'scheduler': self.scheduler.state_dict(),\n",
        "                }\n",
        "\n",
        "                self.save_checkpoint(state)\n",
        "\n",
        "    def save_checkpoint(self, state):\n",
        "        \"\"\"Save checkpoint.\"\"\"\n",
        "        print(\"==> Save checkpoint ...\")\n",
        "        if not os.path.exists(self.ckptroot):\n",
        "            os.makedirs(self.ckptroot)\n",
        "\n",
        "        torch.save(state, self.ckptroot + 'both-nvidia-model-{}.h5'.format(state['epoch']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWQd2Ii-lBQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "353207e1-46a7-476a-9528-0c5fd3336159"
      },
      "source": [
        "print(\"==> Start training ...\")\n",
        "trainer = Trainer(ckptroot,\n",
        "                  model,\n",
        "                  device,\n",
        "                  epochs,\n",
        "                  criterion,\n",
        "                  optimizer,\n",
        "                  scheduler,\n",
        "                  start_epoch,\n",
        "                  trainloader,\n",
        "                  validationloader)\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Start training ...\n",
            "Training Epoch: 0 | Loss: 0.40648894757032394\n",
            "Training Epoch: 0 | Loss: 0.42167221714216885\n",
            "Training Epoch: 0 | Loss: 0.4157953897111155\n",
            "Training Epoch: 0 | Loss: 0.41747473999610374\n",
            "Training Epoch: 0 | Loss: 0.4193281687359486\n",
            "Training Epoch: 0 | Loss: 0.416411739322329\n",
            "Training Epoch: 0 | Loss: 0.4189599215110358\n",
            "Training Epoch: 0 | Loss: 0.41784948914892406\n",
            "Training Epoch: 0 | Loss: 0.41653998980947426\n",
            "Validation Loss: 0.3531637117266655\n",
            "Validation Loss: 0.41932611528894687\n",
            "Validation Loss: 0.4174992385046992\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 1 | Loss: 0.28821399062871933\n",
            "Training Epoch: 1 | Loss: 0.41390728150116335\n",
            "Training Epoch: 1 | Loss: 0.40751984257677304\n",
            "Training Epoch: 1 | Loss: 0.41013384371550377\n",
            "Training Epoch: 1 | Loss: 0.4074074675345287\n",
            "Training Epoch: 1 | Loss: 0.40762388629247687\n",
            "Training Epoch: 1 | Loss: 0.40691767207435187\n",
            "Training Epoch: 1 | Loss: 0.4080559519091661\n",
            "Training Epoch: 1 | Loss: 0.40776879151820317\n",
            "Validation Loss: 0.5099676251411438\n",
            "Validation Loss: 0.4156054145112486\n",
            "Validation Loss: 0.41746329757111583\n",
            "\n",
            "Training Epoch: 2 | Loss: 0.5279360264539719\n",
            "Training Epoch: 2 | Loss: 0.40385313092334435\n",
            "Training Epoch: 2 | Loss: 0.398476887716731\n",
            "Training Epoch: 2 | Loss: 0.39649999905315747\n",
            "Training Epoch: 2 | Loss: 0.3946909111857117\n",
            "Training Epoch: 2 | Loss: 0.39820850578075395\n",
            "Training Epoch: 2 | Loss: 0.39909918659898186\n",
            "Training Epoch: 2 | Loss: 0.40019237005368363\n",
            "Training Epoch: 2 | Loss: 0.39940616224756403\n",
            "Validation Loss: 0.3581196144223213\n",
            "Validation Loss: 0.41114456011074607\n",
            "Validation Loss: 0.4046638267420566\n",
            "\n",
            "Training Epoch: 3 | Loss: 0.3093539588153362\n",
            "Training Epoch: 3 | Loss: 0.41020010157239317\n",
            "Training Epoch: 3 | Loss: 0.39435912253548255\n",
            "Training Epoch: 3 | Loss: 0.39287516294028674\n",
            "Training Epoch: 3 | Loss: 0.3928463390826287\n",
            "Training Epoch: 3 | Loss: 0.393419335070187\n",
            "Training Epoch: 3 | Loss: 0.39387406919851975\n",
            "Training Epoch: 3 | Loss: 0.3948907058090822\n",
            "Training Epoch: 3 | Loss: 0.39401207286422696\n",
            "Validation Loss: 0.5020814165472984\n",
            "Validation Loss: 0.4011388192377468\n",
            "Validation Loss: 0.4023732887719994\n",
            "\n",
            "Training Epoch: 4 | Loss: 0.5073323100805283\n",
            "Training Epoch: 4 | Loss: 0.38419073152512606\n",
            "Training Epoch: 4 | Loss: 0.3843279699632777\n",
            "Training Epoch: 4 | Loss: 0.3882193318831168\n",
            "Training Epoch: 4 | Loss: 0.3869029202607653\n",
            "Training Epoch: 4 | Loss: 0.38495345050078666\n",
            "Training Epoch: 4 | Loss: 0.3857172323319559\n",
            "Training Epoch: 4 | Loss: 0.3836958197379928\n",
            "Training Epoch: 4 | Loss: 0.3850050327064616\n",
            "Validation Loss: 0.42450424283742905\n",
            "Validation Loss: 0.3969554336428052\n",
            "Validation Loss: 0.39932665562451775\n",
            "\n",
            "Training Epoch: 5 | Loss: 0.46515628695487976\n",
            "Training Epoch: 5 | Loss: 0.38144312942824743\n",
            "Training Epoch: 5 | Loss: 0.38109886284861993\n",
            "Training Epoch: 5 | Loss: 0.3816906693097563\n",
            "Training Epoch: 5 | Loss: 0.37971697390042336\n",
            "Training Epoch: 5 | Loss: 0.37862072241104056\n",
            "Training Epoch: 5 | Loss: 0.381440499205508\n",
            "Training Epoch: 5 | Loss: 0.37888016013665055\n",
            "Training Epoch: 5 | Loss: 0.3805529677452108\n",
            "Validation Loss: 0.3866104781627655\n",
            "Validation Loss: 0.3854404136833578\n",
            "Validation Loss: 0.39059058169079064\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 6 | Loss: 0.3640585318207741\n",
            "Training Epoch: 6 | Loss: 0.3708534330719768\n",
            "Training Epoch: 6 | Loss: 0.37861321916553514\n",
            "Training Epoch: 6 | Loss: 0.3776931612446063\n",
            "Training Epoch: 6 | Loss: 0.37600712064755826\n",
            "Training Epoch: 6 | Loss: 0.37540134939247977\n",
            "Training Epoch: 6 | Loss: 0.37547156340543325\n",
            "Training Epoch: 6 | Loss: 0.3766863602671916\n",
            "Training Epoch: 6 | Loss: 0.37589315446407606\n",
            "Validation Loss: 0.44469189643859863\n",
            "Validation Loss: 0.38770606663852636\n",
            "Validation Loss: 0.3809864329757975\n",
            "\n",
            "Training Epoch: 7 | Loss: 0.33478860929608345\n",
            "Training Epoch: 7 | Loss: 0.365700809487907\n",
            "Training Epoch: 7 | Loss: 0.3636455830205139\n",
            "Training Epoch: 7 | Loss: 0.36748835336775876\n",
            "Training Epoch: 7 | Loss: 0.3659514409032397\n",
            "Training Epoch: 7 | Loss: 0.3654051551115727\n",
            "Training Epoch: 7 | Loss: 0.36688590121026243\n",
            "Training Epoch: 7 | Loss: 0.3662210416351678\n",
            "Training Epoch: 7 | Loss: 0.36669693522145985\n",
            "Validation Loss: 0.4245876967906952\n",
            "Validation Loss: 0.3830156190191755\n",
            "Validation Loss: 0.38333040189498396\n",
            "\n",
            "Training Epoch: 8 | Loss: 0.46410317718982697\n",
            "Training Epoch: 8 | Loss: 0.38044341956034744\n",
            "Training Epoch: 8 | Loss: 0.36852514188718144\n",
            "Training Epoch: 8 | Loss: 0.3653651264084633\n",
            "Training Epoch: 8 | Loss: 0.3630641963555851\n",
            "Training Epoch: 8 | Loss: 0.3654164506267466\n",
            "Training Epoch: 8 | Loss: 0.36473867452134706\n",
            "Training Epoch: 8 | Loss: 0.3622780396854146\n",
            "Training Epoch: 8 | Loss: 0.3632828018601319\n",
            "Validation Loss: 0.47447066754102707\n",
            "Validation Loss: 0.3790956422096432\n",
            "Validation Loss: 0.38064290466370865\n",
            "\n",
            "Training Epoch: 9 | Loss: 0.26424451917409897\n",
            "Training Epoch: 9 | Loss: 0.355402964250286\n",
            "Training Epoch: 9 | Loss: 0.35730562969782753\n",
            "Training Epoch: 9 | Loss: 0.357874037623653\n",
            "Training Epoch: 9 | Loss: 0.3580080631079891\n",
            "Training Epoch: 9 | Loss: 0.35554754097170815\n",
            "Training Epoch: 9 | Loss: 0.3532956933010735\n",
            "Training Epoch: 9 | Loss: 0.355642500005333\n",
            "Training Epoch: 9 | Loss: 0.35560667097856863\n",
            "Validation Loss: 0.34126250445842743\n",
            "Validation Loss: 0.37702395280103873\n",
            "Validation Loss: 0.3783285830410855\n",
            "\n",
            "Training Epoch: 10 | Loss: 0.3711286894977093\n",
            "Training Epoch: 10 | Loss: 0.3563083469830822\n",
            "Training Epoch: 10 | Loss: 0.35239351703901195\n",
            "Training Epoch: 10 | Loss: 0.34945971277415555\n",
            "Training Epoch: 10 | Loss: 0.3487436592839008\n",
            "Training Epoch: 10 | Loss: 0.34860175156114465\n",
            "Training Epoch: 10 | Loss: 0.3512196617476803\n",
            "Training Epoch: 10 | Loss: 0.35074887624257556\n",
            "Training Epoch: 10 | Loss: 0.35256288872192265\n",
            "Validation Loss: 0.3754289150238037\n",
            "Validation Loss: 0.37738367354515756\n",
            "Validation Loss: 0.3774204946572508\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 11 | Loss: 0.36618978157639503\n",
            "Training Epoch: 11 | Loss: 0.3441673167935102\n",
            "Training Epoch: 11 | Loss: 0.34298818514900126\n",
            "Training Epoch: 11 | Loss: 0.34121959415756387\n",
            "Training Epoch: 11 | Loss: 0.34153028396411134\n",
            "Training Epoch: 11 | Loss: 0.3430274026524164\n",
            "Training Epoch: 11 | Loss: 0.34378505560775563\n",
            "Training Epoch: 11 | Loss: 0.34588530427974234\n",
            "Training Epoch: 11 | Loss: 0.3461722944085294\n",
            "Validation Loss: 0.24106362462043762\n",
            "Validation Loss: 0.3604841079968627\n",
            "Validation Loss: 0.36881270058527216\n",
            "\n",
            "Training Epoch: 12 | Loss: 0.34768592566251755\n",
            "Training Epoch: 12 | Loss: 0.34167266192633916\n",
            "Training Epoch: 12 | Loss: 0.34063143857676\n",
            "Training Epoch: 12 | Loss: 0.3357918392928732\n",
            "Training Epoch: 12 | Loss: 0.338193516611607\n",
            "Training Epoch: 12 | Loss: 0.3404577281586305\n",
            "Training Epoch: 12 | Loss: 0.341157213163356\n",
            "Training Epoch: 12 | Loss: 0.3419849414766345\n",
            "Training Epoch: 12 | Loss: 0.34329549648732727\n",
            "Validation Loss: 0.38957472145557404\n",
            "Validation Loss: 0.37921610334427047\n",
            "Validation Loss: 0.3800286085747961\n",
            "\n",
            "Training Epoch: 13 | Loss: 0.4172022193670273\n",
            "Training Epoch: 13 | Loss: 0.3354897611765283\n",
            "Training Epoch: 13 | Loss: 0.3317538361477466\n",
            "Training Epoch: 13 | Loss: 0.3348640418020495\n",
            "Training Epoch: 13 | Loss: 0.3335166172893193\n",
            "Training Epoch: 13 | Loss: 0.3364110310374203\n",
            "Training Epoch: 13 | Loss: 0.3369248824135188\n",
            "Training Epoch: 13 | Loss: 0.3375380834333397\n",
            "Training Epoch: 13 | Loss: 0.3367618568306707\n",
            "Validation Loss: 0.40639493614435196\n",
            "Validation Loss: 0.3823499951032129\n",
            "Validation Loss: 0.3737517057626105\n",
            "\n",
            "Training Epoch: 14 | Loss: 0.2946626581251621\n",
            "Training Epoch: 14 | Loss: 0.3303206039979906\n",
            "Training Epoch: 14 | Loss: 0.3316161314312795\n",
            "Training Epoch: 14 | Loss: 0.332792219455258\n",
            "Training Epoch: 14 | Loss: 0.3311206252776328\n",
            "Training Epoch: 14 | Loss: 0.3287979260295213\n",
            "Training Epoch: 14 | Loss: 0.33022018334829867\n",
            "Training Epoch: 14 | Loss: 0.33091558193450477\n",
            "Training Epoch: 14 | Loss: 0.3317890805046954\n",
            "Validation Loss: 0.3210204169154167\n",
            "Validation Loss: 0.36639580960468493\n",
            "Validation Loss: 0.3625538016377545\n",
            "\n",
            "Training Epoch: 15 | Loss: 0.3545471280813217\n",
            "Training Epoch: 15 | Loss: 0.32059207593671757\n",
            "Training Epoch: 15 | Loss: 0.31937851296246644\n",
            "Training Epoch: 15 | Loss: 0.3205759286806235\n",
            "Training Epoch: 15 | Loss: 0.3252591362032584\n",
            "Training Epoch: 15 | Loss: 0.3244865882487414\n",
            "Training Epoch: 15 | Loss: 0.32404744050181944\n",
            "Training Epoch: 15 | Loss: 0.3256589360854\n",
            "Training Epoch: 15 | Loss: 0.3270883571947186\n",
            "Validation Loss: 0.41608723998069763\n",
            "Validation Loss: 0.3608979923964137\n",
            "Validation Loss: 0.3645656681179407\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 16 | Loss: 0.3793894946575165\n",
            "Training Epoch: 16 | Loss: 0.3187910504085888\n",
            "Training Epoch: 16 | Loss: 0.3209958712185793\n",
            "Training Epoch: 16 | Loss: 0.3195938227382609\n",
            "Training Epoch: 16 | Loss: 0.3218265187060298\n",
            "Training Epoch: 16 | Loss: 0.32542726863733307\n",
            "Training Epoch: 16 | Loss: 0.32828796190673026\n",
            "Training Epoch: 16 | Loss: 0.32689309257551624\n",
            "Training Epoch: 16 | Loss: 0.325053280803111\n",
            "Validation Loss: 0.3380989134311676\n",
            "Validation Loss: 0.355927417405171\n",
            "Validation Loss: 0.35622198045698567\n",
            "\n",
            "Training Epoch: 17 | Loss: 0.21626066416502\n",
            "Training Epoch: 17 | Loss: 0.31639414900305246\n",
            "Training Epoch: 17 | Loss: 0.3203856298116161\n",
            "Training Epoch: 17 | Loss: 0.32073154442779644\n",
            "Training Epoch: 17 | Loss: 0.3218182575589329\n",
            "Training Epoch: 17 | Loss: 0.3202885794536975\n",
            "Training Epoch: 17 | Loss: 0.31894047273487197\n",
            "Training Epoch: 17 | Loss: 0.3174585234267523\n",
            "Training Epoch: 17 | Loss: 0.31780278941060647\n",
            "Validation Loss: 0.33021651208400726\n",
            "Validation Loss: 0.37038616013556425\n",
            "Validation Loss: 0.3646793853658349\n",
            "\n",
            "Training Epoch: 18 | Loss: 0.25994451344013214\n",
            "Training Epoch: 18 | Loss: 0.3231685485617064\n",
            "Training Epoch: 18 | Loss: 0.320159076291382\n",
            "Training Epoch: 18 | Loss: 0.31863949184253365\n",
            "Training Epoch: 18 | Loss: 0.31835605528790906\n",
            "Training Epoch: 18 | Loss: 0.316725332780691\n",
            "Training Epoch: 18 | Loss: 0.31772350071125144\n",
            "Training Epoch: 18 | Loss: 0.31634390093226744\n",
            "Training Epoch: 18 | Loss: 0.3151376010447778\n",
            "Validation Loss: 0.5478845983743668\n",
            "Validation Loss: 0.3586061595026219\n",
            "Validation Loss: 0.3610477239004712\n",
            "\n",
            "Training Epoch: 19 | Loss: 0.3541312515735626\n",
            "Training Epoch: 19 | Loss: 0.3050057773690413\n",
            "Training Epoch: 19 | Loss: 0.2990844193104636\n",
            "Training Epoch: 19 | Loss: 0.30261967337978046\n",
            "Training Epoch: 19 | Loss: 0.30566896732775795\n",
            "Training Epoch: 19 | Loss: 0.3067849053333917\n",
            "Training Epoch: 19 | Loss: 0.3099517520430332\n",
            "Training Epoch: 19 | Loss: 0.3118786524885604\n",
            "Training Epoch: 19 | Loss: 0.3121522828359878\n",
            "Validation Loss: 0.3290228545665741\n",
            "Validation Loss: 0.3567021040633173\n",
            "Validation Loss: 0.3476910539508904\n",
            "\n",
            "Training Epoch: 20 | Loss: 0.31059640273451805\n",
            "Training Epoch: 20 | Loss: 0.29685057802955706\n",
            "Training Epoch: 20 | Loss: 0.2973901074083142\n",
            "Training Epoch: 20 | Loss: 0.29477619191019044\n",
            "Training Epoch: 20 | Loss: 0.29278308881488524\n",
            "Training Epoch: 20 | Loss: 0.2923766259446056\n",
            "Training Epoch: 20 | Loss: 0.2932101660369339\n",
            "Training Epoch: 20 | Loss: 0.29151439628751147\n",
            "Training Epoch: 20 | Loss: 0.29016035546394026\n",
            "Validation Loss: 0.3949578106403351\n",
            "Validation Loss: 0.3287501192033881\n",
            "Validation Loss: 0.3327031278176539\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 21 | Loss: 0.23870451003313065\n",
            "Training Epoch: 21 | Loss: 0.28974912059262836\n",
            "Training Epoch: 21 | Loss: 0.2846747209128011\n",
            "Training Epoch: 21 | Loss: 0.2826362165848282\n",
            "Training Epoch: 21 | Loss: 0.2842393161733623\n",
            "Training Epoch: 21 | Loss: 0.28344078982869786\n",
            "Training Epoch: 21 | Loss: 0.28270463822425007\n",
            "Training Epoch: 21 | Loss: 0.28135066430287675\n",
            "Training Epoch: 21 | Loss: 0.28086679603220815\n",
            "Validation Loss: 0.2536972798407078\n",
            "Validation Loss: 0.32835392644721095\n",
            "Validation Loss: 0.3284086862866262\n",
            "\n",
            "Training Epoch: 22 | Loss: 0.2776998095214367\n",
            "Training Epoch: 22 | Loss: 0.27305991268984164\n",
            "Training Epoch: 22 | Loss: 0.27309869350263133\n",
            "Training Epoch: 22 | Loss: 0.2769540171216295\n",
            "Training Epoch: 22 | Loss: 0.27629141924331463\n",
            "Training Epoch: 22 | Loss: 0.27808483570494397\n",
            "Training Epoch: 22 | Loss: 0.2785424657038935\n",
            "Training Epoch: 22 | Loss: 0.27783045174418475\n",
            "Training Epoch: 22 | Loss: 0.27811816439483683\n",
            "Validation Loss: 0.2712450549006462\n",
            "Validation Loss: 0.32128427269877774\n",
            "Validation Loss: 0.32844594361918483\n",
            "\n",
            "Training Epoch: 23 | Loss: 0.22099346667528152\n",
            "Training Epoch: 23 | Loss: 0.28216046022970487\n",
            "Training Epoch: 23 | Loss: 0.28334834500776596\n",
            "Training Epoch: 23 | Loss: 0.28046542907325533\n",
            "Training Epoch: 23 | Loss: 0.2786741630822197\n",
            "Training Epoch: 23 | Loss: 0.27795995697527826\n",
            "Training Epoch: 23 | Loss: 0.27687948976586046\n",
            "Training Epoch: 23 | Loss: 0.2778269281762303\n",
            "Training Epoch: 23 | Loss: 0.276352978447515\n",
            "Validation Loss: 0.49536171555519104\n",
            "Validation Loss: 0.3224852748657807\n",
            "Validation Loss: 0.3245846468640204\n",
            "\n",
            "Training Epoch: 24 | Loss: 0.3593572676181793\n",
            "Training Epoch: 24 | Loss: 0.2767329955160028\n",
            "Training Epoch: 24 | Loss: 0.27030096942586684\n",
            "Training Epoch: 24 | Loss: 0.27119668337989883\n",
            "Training Epoch: 24 | Loss: 0.2729380312805387\n",
            "Training Epoch: 24 | Loss: 0.272950281379763\n",
            "Training Epoch: 24 | Loss: 0.27279144109919445\n",
            "Training Epoch: 24 | Loss: 0.27287945076663805\n",
            "Training Epoch: 24 | Loss: 0.2731617197310657\n",
            "Validation Loss: 0.3868632912635803\n",
            "Validation Loss: 0.32163111421067525\n",
            "Validation Loss: 0.32690951406992785\n",
            "\n",
            "Training Epoch: 25 | Loss: 0.22964783757925034\n",
            "Training Epoch: 25 | Loss: 0.27324917894041184\n",
            "Training Epoch: 25 | Loss: 0.27048945612278746\n",
            "Training Epoch: 25 | Loss: 0.2691753301088794\n",
            "Training Epoch: 25 | Loss: 0.2702183908924572\n",
            "Training Epoch: 25 | Loss: 0.27063140563086835\n",
            "Training Epoch: 25 | Loss: 0.2704985357988347\n",
            "Training Epoch: 25 | Loss: 0.27092252993496524\n",
            "Training Epoch: 25 | Loss: 0.2706476506008563\n",
            "Validation Loss: 0.2551315352320671\n",
            "Validation Loss: 0.3292722331710381\n",
            "Validation Loss: 0.3257164730695053\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 26 | Loss: 0.24889516085386276\n",
            "Training Epoch: 26 | Loss: 0.27264168830865093\n",
            "Training Epoch: 26 | Loss: 0.26988515913931294\n",
            "Training Epoch: 26 | Loss: 0.2698037630237911\n",
            "Training Epoch: 26 | Loss: 0.27130228391572125\n",
            "Training Epoch: 26 | Loss: 0.26967714353652295\n",
            "Training Epoch: 26 | Loss: 0.27094554000792903\n",
            "Training Epoch: 26 | Loss: 0.2711652790099255\n",
            "Training Epoch: 26 | Loss: 0.27029358017515304\n",
            "Validation Loss: 0.33373839035630226\n",
            "Validation Loss: 0.3288115078196077\n",
            "Validation Loss: 0.3251567476832155\n",
            "\n",
            "Training Epoch: 27 | Loss: 0.2200683280825615\n",
            "Training Epoch: 27 | Loss: 0.2656589203528059\n",
            "Training Epoch: 27 | Loss: 0.26978568128771163\n",
            "Training Epoch: 27 | Loss: 0.2699178712224202\n",
            "Training Epoch: 27 | Loss: 0.269053921806152\n",
            "Training Epoch: 27 | Loss: 0.26975095756165995\n",
            "Training Epoch: 27 | Loss: 0.26874646069893826\n",
            "Training Epoch: 27 | Loss: 0.26883977576485657\n",
            "Validation Loss: 0.25908520817756653\n",
            "Validation Loss: 0.31468545774569606\n",
            "Validation Loss: 0.3226140684712289\n",
            "\n",
            "Training Epoch: 28 | Loss: 0.3241065591573715\n",
            "Training Epoch: 28 | Loss: 0.2682913950653655\n",
            "Training Epoch: 28 | Loss: 0.26720247155435345\n",
            "Training Epoch: 28 | Loss: 0.268763580245037\n",
            "Training Epoch: 28 | Loss: 0.2696739098712096\n",
            "Training Epoch: 28 | Loss: 0.26824735410645334\n",
            "Training Epoch: 28 | Loss: 0.26954742358337325\n",
            "Training Epoch: 28 | Loss: 0.2698045033541709\n",
            "Training Epoch: 28 | Loss: 0.2684241988476408\n",
            "Validation Loss: 0.29712435603141785\n",
            "Validation Loss: 0.3144796679146809\n",
            "Validation Loss: 0.32177549995370763\n",
            "\n",
            "Training Epoch: 29 | Loss: 0.2659916654229164\n",
            "Training Epoch: 29 | Loss: 0.26553675290750395\n",
            "Training Epoch: 29 | Loss: 0.2643082883227524\n",
            "Training Epoch: 29 | Loss: 0.26408366341007705\n",
            "Training Epoch: 29 | Loss: 0.26442289533572005\n",
            "Training Epoch: 29 | Loss: 0.26621657353779393\n",
            "Training Epoch: 29 | Loss: 0.26646789165830453\n",
            "Training Epoch: 29 | Loss: 0.26745172842444265\n",
            "Training Epoch: 29 | Loss: 0.26693137043321996\n",
            "Validation Loss: 0.44493211805820465\n",
            "Validation Loss: 0.3227406802153823\n",
            "Validation Loss: 0.3219018556659494\n",
            "\n",
            "Training Epoch: 30 | Loss: 0.26440633088350296\n",
            "Training Epoch: 30 | Loss: 0.26979321068023693\n",
            "Training Epoch: 30 | Loss: 0.2688397961339696\n",
            "Training Epoch: 30 | Loss: 0.26692633180448977\n",
            "Training Epoch: 30 | Loss: 0.26763267548954545\n",
            "Training Epoch: 30 | Loss: 0.26834055905168996\n",
            "Training Epoch: 30 | Loss: 0.26625912822782993\n",
            "Training Epoch: 30 | Loss: 0.2668443099328565\n",
            "Training Epoch: 30 | Loss: 0.2662624098248919\n",
            "Validation Loss: 0.3435848280787468\n",
            "Validation Loss: 0.31840200596811746\n",
            "Validation Loss: 0.32083555612138553\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 31 | Loss: 0.18919779360294342\n",
            "Training Epoch: 31 | Loss: 0.267372788308133\n",
            "Training Epoch: 31 | Loss: 0.26672661672369463\n",
            "Training Epoch: 31 | Loss: 0.2636364859798026\n",
            "Training Epoch: 31 | Loss: 0.2660408202660946\n",
            "Training Epoch: 31 | Loss: 0.2650720360028946\n",
            "Training Epoch: 31 | Loss: 0.2658973182683281\n",
            "Training Epoch: 31 | Loss: 0.26569138391318065\n",
            "Training Epoch: 31 | Loss: 0.26551984328446393\n",
            "Validation Loss: 0.3135532736778259\n",
            "Validation Loss: 0.3258896574820622\n",
            "Validation Loss: 0.32088755417744913\n",
            "\n",
            "Training Epoch: 32 | Loss: 0.3211233615875244\n",
            "Training Epoch: 32 | Loss: 0.27191979670436073\n",
            "Training Epoch: 32 | Loss: 0.2696111946093354\n",
            "Training Epoch: 32 | Loss: 0.2681289878596499\n",
            "Training Epoch: 32 | Loss: 0.2659109305646131\n",
            "Training Epoch: 32 | Loss: 0.26664521347381875\n",
            "Training Epoch: 32 | Loss: 0.26629334629301027\n",
            "Training Epoch: 32 | Loss: 0.26623064747210406\n",
            "Training Epoch: 32 | Loss: 0.26518022977792366\n",
            "Validation Loss: 0.30018504709005356\n",
            "Validation Loss: 0.31571686166421614\n",
            "Validation Loss: 0.319328389403906\n",
            "\n",
            "Training Epoch: 33 | Loss: 0.29968947172164917\n",
            "Training Epoch: 33 | Loss: 0.2670778251336058\n",
            "Training Epoch: 33 | Loss: 0.2709483059947912\n",
            "Training Epoch: 33 | Loss: 0.26954618650194617\n",
            "Training Epoch: 33 | Loss: 0.2687211876544646\n",
            "Training Epoch: 33 | Loss: 0.2663636956096231\n",
            "Training Epoch: 33 | Loss: 0.2651364294325502\n",
            "Training Epoch: 33 | Loss: 0.2635735910061678\n",
            "Training Epoch: 33 | Loss: 0.26346951121982937\n",
            "Validation Loss: 0.3063768595457077\n",
            "Validation Loss: 0.3168621070240394\n",
            "Validation Loss: 0.3169649450099142\n",
            "\n",
            "Training Epoch: 34 | Loss: 0.24005386978387833\n",
            "Training Epoch: 34 | Loss: 0.27148014679551125\n",
            "Training Epoch: 34 | Loss: 0.2661182521216905\n",
            "Training Epoch: 34 | Loss: 0.2630867730838516\n",
            "Training Epoch: 34 | Loss: 0.2636385544985904\n",
            "Training Epoch: 34 | Loss: 0.26342576227992953\n",
            "Training Epoch: 34 | Loss: 0.26272685249055583\n",
            "Training Epoch: 34 | Loss: 0.2622777928177071\n",
            "Training Epoch: 34 | Loss: 0.26185772688210607\n",
            "Validation Loss: 0.2636908143758774\n",
            "Validation Loss: 0.31196318411886104\n",
            "Validation Loss: 0.31652737123456165\n",
            "\n",
            "Training Epoch: 35 | Loss: 0.30150844156742096\n",
            "Training Epoch: 35 | Loss: 0.2645309774371067\n",
            "Training Epoch: 35 | Loss: 0.26115307129400583\n",
            "Training Epoch: 35 | Loss: 0.2586376410645999\n",
            "Training Epoch: 35 | Loss: 0.2619026116608756\n",
            "Training Epoch: 35 | Loss: 0.26233395938566345\n",
            "Training Epoch: 35 | Loss: 0.2640356701642225\n",
            "Training Epoch: 35 | Loss: 0.2627803160066314\n",
            "Training Epoch: 35 | Loss: 0.2617231548116188\n",
            "Validation Loss: 0.36140765249729156\n",
            "Validation Loss: 0.3175317921703405\n",
            "Validation Loss: 0.3187411562331132\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 36 | Loss: 0.22307265922427177\n",
            "Training Epoch: 36 | Loss: 0.265099092432768\n",
            "Training Epoch: 36 | Loss: 0.2605014747629563\n",
            "Training Epoch: 36 | Loss: 0.2617750913337913\n",
            "Training Epoch: 36 | Loss: 0.26162939191682083\n",
            "Training Epoch: 36 | Loss: 0.2622972918105518\n",
            "Training Epoch: 36 | Loss: 0.2617773800412698\n",
            "Training Epoch: 36 | Loss: 0.2633404532970911\n",
            "Training Epoch: 36 | Loss: 0.2631197879916422\n",
            "Validation Loss: 0.4725925549864769\n",
            "Validation Loss: 0.3181480932427515\n",
            "Validation Loss: 0.3179593541432376\n",
            "\n",
            "Training Epoch: 37 | Loss: 0.24709944799542427\n",
            "Training Epoch: 37 | Loss: 0.25634844400136186\n",
            "Training Epoch: 37 | Loss: 0.2537741014984117\n",
            "Training Epoch: 37 | Loss: 0.26017195891056744\n",
            "Training Epoch: 37 | Loss: 0.2592124624591219\n",
            "Training Epoch: 37 | Loss: 0.2597704024394294\n",
            "Training Epoch: 37 | Loss: 0.2598482534884415\n",
            "Training Epoch: 37 | Loss: 0.2589794909143116\n",
            "Training Epoch: 37 | Loss: 0.259843304127771\n",
            "Validation Loss: 0.30061452835798264\n",
            "Validation Loss: 0.3257171576934876\n",
            "Validation Loss: 0.31806823043199023\n",
            "\n",
            "Training Epoch: 38 | Loss: 0.23363658785820007\n",
            "Training Epoch: 38 | Loss: 0.2637224417401127\n",
            "Training Epoch: 38 | Loss: 0.26098965723716205\n",
            "Training Epoch: 38 | Loss: 0.26254961298054635\n",
            "Training Epoch: 38 | Loss: 0.2603387390530466\n",
            "Training Epoch: 38 | Loss: 0.25872231456646305\n",
            "Training Epoch: 38 | Loss: 0.2588854553069479\n",
            "Training Epoch: 38 | Loss: 0.25763704352474076\n",
            "Training Epoch: 38 | Loss: 0.25765309842552214\n",
            "Validation Loss: 0.3066880702972412\n",
            "Validation Loss: 0.3163224702866951\n",
            "Validation Loss: 0.3154646943402083\n",
            "\n",
            "Training Epoch: 39 | Loss: 0.3153328076004982\n",
            "Training Epoch: 39 | Loss: 0.2637808269609024\n",
            "Training Epoch: 39 | Loss: 0.259725633582369\n",
            "Training Epoch: 39 | Loss: 0.25791774909831955\n",
            "Training Epoch: 39 | Loss: 0.26056402496659103\n",
            "Training Epoch: 39 | Loss: 0.25811054870233446\n",
            "Training Epoch: 39 | Loss: 0.25811925941083375\n",
            "Training Epoch: 39 | Loss: 0.2588154348955302\n",
            "Training Epoch: 39 | Loss: 0.25874282529672954\n",
            "Validation Loss: 0.38202938437461853\n",
            "Validation Loss: 0.31396042990802536\n",
            "Validation Loss: 0.31900899231767477\n",
            "\n",
            "Training Epoch: 40 | Loss: 0.2350366786122322\n",
            "Training Epoch: 40 | Loss: 0.2612845982876745\n",
            "Training Epoch: 40 | Loss: 0.25930172169757126\n",
            "Training Epoch: 40 | Loss: 0.2586051802772818\n",
            "Training Epoch: 40 | Loss: 0.2584569170848093\n",
            "Training Epoch: 40 | Loss: 0.2572284786517689\n",
            "Training Epoch: 40 | Loss: 0.25606568620389886\n",
            "Training Epoch: 40 | Loss: 0.25656764119991055\n",
            "Training Epoch: 40 | Loss: 0.2571937549910444\n",
            "Validation Loss: 0.3739396035671234\n",
            "Validation Loss: 0.3187110862408829\n",
            "Validation Loss: 0.31693843402783967\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 41 | Loss: 0.24850061535835266\n",
            "Training Epoch: 41 | Loss: 0.2590091700232265\n",
            "Training Epoch: 41 | Loss: 0.260626394551859\n",
            "Training Epoch: 41 | Loss: 0.2602316356539429\n",
            "Training Epoch: 41 | Loss: 0.2579653788929308\n",
            "Training Epoch: 41 | Loss: 0.25786621578803615\n",
            "Training Epoch: 41 | Loss: 0.2563079554950214\n",
            "Training Epoch: 41 | Loss: 0.2553298984815004\n",
            "Training Epoch: 41 | Loss: 0.25612824669598455\n",
            "Validation Loss: 0.3942975252866745\n",
            "Validation Loss: 0.3209353021465906\n",
            "Validation Loss: 0.3156377301268761\n",
            "\n",
            "Training Epoch: 42 | Loss: 0.25141094624996185\n",
            "Training Epoch: 42 | Loss: 0.25469777458301274\n",
            "Training Epoch: 42 | Loss: 0.26002276430267895\n",
            "Training Epoch: 42 | Loss: 0.2587887135541617\n",
            "Training Epoch: 42 | Loss: 0.2565992771510843\n",
            "Training Epoch: 42 | Loss: 0.25607885210635417\n",
            "Training Epoch: 42 | Loss: 0.25566975715974605\n",
            "Training Epoch: 42 | Loss: 0.25505753796276626\n",
            "Training Epoch: 42 | Loss: 0.25439297014175355\n",
            "Validation Loss: 0.1431085504591465\n",
            "Validation Loss: 0.31201662475446074\n",
            "Validation Loss: 0.3164106058095818\n",
            "\n",
            "Training Epoch: 43 | Loss: 0.2248658761382103\n",
            "Training Epoch: 43 | Loss: 0.2521326879850856\n",
            "Training Epoch: 43 | Loss: 0.2502451527780339\n",
            "Training Epoch: 43 | Loss: 0.2521478721587315\n",
            "Training Epoch: 43 | Loss: 0.2524029512232416\n",
            "Training Epoch: 43 | Loss: 0.25375751381230627\n",
            "Training Epoch: 43 | Loss: 0.2534303833279579\n",
            "Training Epoch: 43 | Loss: 0.2545359963498021\n",
            "Training Epoch: 43 | Loss: 0.25502102744048194\n",
            "Validation Loss: 0.41196344047784805\n",
            "Validation Loss: 0.31757863942939457\n",
            "Validation Loss: 0.3127603707881413\n",
            "\n",
            "Training Epoch: 44 | Loss: 0.2603408806025982\n",
            "Training Epoch: 44 | Loss: 0.2500759737363251\n",
            "Training Epoch: 44 | Loss: 0.24982478865306473\n",
            "Training Epoch: 44 | Loss: 0.25160998875194807\n",
            "Training Epoch: 44 | Loss: 0.2527870397336614\n",
            "Training Epoch: 44 | Loss: 0.2541108799767411\n",
            "Training Epoch: 44 | Loss: 0.2539387050947909\n",
            "Training Epoch: 44 | Loss: 0.25453599526436627\n",
            "Training Epoch: 44 | Loss: 0.2542597994129272\n",
            "Validation Loss: 0.32672926783561707\n",
            "Validation Loss: 0.3126444509345116\n",
            "Validation Loss: 0.31455668530876363\n",
            "\n",
            "Training Epoch: 45 | Loss: 0.18951759859919548\n",
            "Training Epoch: 45 | Loss: 0.25581372060840674\n",
            "Training Epoch: 45 | Loss: 0.24963603339236767\n",
            "Training Epoch: 45 | Loss: 0.25194270849425926\n",
            "Training Epoch: 45 | Loss: 0.2539295061094282\n",
            "Training Epoch: 45 | Loss: 0.2531626830862727\n",
            "Training Epoch: 45 | Loss: 0.25426594517232476\n",
            "Training Epoch: 45 | Loss: 0.25375847665648826\n",
            "Training Epoch: 45 | Loss: 0.25471188963743185\n",
            "Validation Loss: 0.4496493935585022\n",
            "Validation Loss: 0.3162074545674985\n",
            "Validation Loss: 0.3145578149984132\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 46 | Loss: 0.2328170109540224\n",
            "Training Epoch: 46 | Loss: 0.2472301188598175\n",
            "Training Epoch: 46 | Loss: 0.2484928789404939\n",
            "Training Epoch: 46 | Loss: 0.2506496412772376\n",
            "Training Epoch: 46 | Loss: 0.2534245368734262\n",
            "Training Epoch: 46 | Loss: 0.253541540540383\n",
            "Training Epoch: 46 | Loss: 0.25355038850470607\n",
            "Training Epoch: 46 | Loss: 0.25371811233099534\n",
            "Training Epoch: 46 | Loss: 0.25406950751968316\n",
            "Validation Loss: 0.28398597240448\n",
            "Validation Loss: 0.3097588173528709\n",
            "Validation Loss: 0.3143080243497939\n",
            "\n",
            "Training Epoch: 47 | Loss: 0.3008662573993206\n",
            "Training Epoch: 47 | Loss: 0.24784603764736415\n",
            "Training Epoch: 47 | Loss: 0.252629065876873\n",
            "Training Epoch: 47 | Loss: 0.25215912927746575\n",
            "Training Epoch: 47 | Loss: 0.25300739394347566\n",
            "Training Epoch: 47 | Loss: 0.2558967805060441\n",
            "Training Epoch: 47 | Loss: 0.25506381040590875\n",
            "Training Epoch: 47 | Loss: 0.255245066227952\n",
            "Training Epoch: 47 | Loss: 0.2549868682713321\n",
            "Validation Loss: 0.29102761298418045\n",
            "Validation Loss: 0.3061281555323022\n",
            "Validation Loss: 0.3163897124633415\n",
            "\n",
            "Training Epoch: 48 | Loss: 0.2542458809912205\n",
            "Training Epoch: 48 | Loss: 0.24608747109697007\n",
            "Training Epoch: 48 | Loss: 0.2521050676184507\n",
            "Training Epoch: 48 | Loss: 0.25417802528017186\n",
            "Training Epoch: 48 | Loss: 0.25380443187983254\n",
            "Training Epoch: 48 | Loss: 0.253599892673408\n",
            "Training Epoch: 48 | Loss: 0.25384898883555573\n",
            "Training Epoch: 48 | Loss: 0.2540899710716433\n",
            "Training Epoch: 48 | Loss: 0.25494745924613077\n",
            "Validation Loss: 0.20716317743062973\n",
            "Validation Loss: 0.31853564935598044\n",
            "Validation Loss: 0.31488464248195214\n",
            "\n",
            "Training Epoch: 49 | Loss: 0.19328673183918\n",
            "Training Epoch: 49 | Loss: 0.25518648807733957\n",
            "Training Epoch: 49 | Loss: 0.2539240233322133\n",
            "Training Epoch: 49 | Loss: 0.25444548476202744\n",
            "Training Epoch: 49 | Loss: 0.2534250785083396\n",
            "Training Epoch: 49 | Loss: 0.25144414695541184\n",
            "Training Epoch: 49 | Loss: 0.25219464852488577\n",
            "Training Epoch: 49 | Loss: 0.25261344336047664\n",
            "Training Epoch: 49 | Loss: 0.25414366210193445\n",
            "Validation Loss: 0.296678364276886\n",
            "Validation Loss: 0.3166472603852796\n",
            "Validation Loss: 0.31441100424535534\n",
            "\n",
            "Training Epoch: 50 | Loss: 0.19642215222120285\n",
            "Training Epoch: 50 | Loss: 0.25648834740761484\n",
            "Training Epoch: 50 | Loss: 0.25730188157576234\n",
            "Training Epoch: 50 | Loss: 0.2534924582129598\n",
            "Training Epoch: 50 | Loss: 0.25380970006971215\n",
            "Training Epoch: 50 | Loss: 0.25072678182080776\n",
            "Training Epoch: 50 | Loss: 0.25293015620009524\n",
            "Training Epoch: 50 | Loss: 0.2544817641263193\n",
            "Training Epoch: 50 | Loss: 0.25435145122444397\n",
            "Validation Loss: 0.4511970579624176\n",
            "Validation Loss: 0.3191311535803546\n",
            "Validation Loss: 0.3154894563216549\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 51 | Loss: 0.38815658539533615\n",
            "Training Epoch: 51 | Loss: 0.2550744738724857\n",
            "Training Epoch: 51 | Loss: 0.25499845285950906\n",
            "Training Epoch: 51 | Loss: 0.25132404282949\n",
            "Training Epoch: 51 | Loss: 0.250727385623924\n",
            "Training Epoch: 51 | Loss: 0.2513582592480584\n",
            "Training Epoch: 51 | Loss: 0.25191526152906374\n",
            "Training Epoch: 51 | Loss: 0.2523931222908018\n",
            "Training Epoch: 51 | Loss: 0.2533996773502335\n",
            "Validation Loss: 0.3273342028260231\n",
            "Validation Loss: 0.3221357910903079\n",
            "Validation Loss: 0.3176427704388674\n",
            "\n",
            "Training Epoch: 52 | Loss: 0.1724010705947876\n",
            "Training Epoch: 52 | Loss: 0.25059965348774843\n",
            "Training Epoch: 52 | Loss: 0.2535986357567767\n",
            "Training Epoch: 52 | Loss: 0.25543527141757977\n",
            "Training Epoch: 52 | Loss: 0.2549503044157923\n",
            "Training Epoch: 52 | Loss: 0.2549610905007212\n",
            "Training Epoch: 52 | Loss: 0.25493681008389807\n",
            "Training Epoch: 52 | Loss: 0.25420714988135407\n",
            "Training Epoch: 52 | Loss: 0.25442955402492284\n",
            "Validation Loss: 0.33929890394210815\n",
            "Validation Loss: 0.3198208873077194\n",
            "Validation Loss: 0.3115553529330747\n",
            "\n",
            "Training Epoch: 53 | Loss: 0.16825774684548378\n",
            "Training Epoch: 53 | Loss: 0.2600696371997347\n",
            "Training Epoch: 53 | Loss: 0.2585915598499389\n",
            "Training Epoch: 53 | Loss: 0.25721423665179166\n",
            "Training Epoch: 53 | Loss: 0.2576838422780322\n",
            "Training Epoch: 53 | Loss: 0.2577562865330847\n",
            "Training Epoch: 53 | Loss: 0.2559456467169791\n",
            "Training Epoch: 53 | Loss: 0.2546059707931632\n",
            "Training Epoch: 53 | Loss: 0.25306814295540886\n",
            "Validation Loss: 0.24560809880495071\n",
            "Validation Loss: 0.3169564838604172\n",
            "Validation Loss: 0.31353141209898305\n",
            "\n",
            "Training Epoch: 54 | Loss: 0.26104895398020744\n",
            "Training Epoch: 54 | Loss: 0.25968839331428606\n",
            "Training Epoch: 54 | Loss: 0.2530984395549665\n",
            "Training Epoch: 54 | Loss: 0.25306049718883544\n",
            "Training Epoch: 54 | Loss: 0.2531519690319487\n",
            "Training Epoch: 54 | Loss: 0.25344990183433846\n",
            "Training Epoch: 54 | Loss: 0.25363126487010923\n",
            "Training Epoch: 54 | Loss: 0.2534158141398736\n",
            "Training Epoch: 54 | Loss: 0.25348550703064926\n",
            "Validation Loss: 0.2929559126496315\n",
            "Validation Loss: 0.3175703631252936\n",
            "Validation Loss: 0.3123287283884945\n",
            "\n",
            "Training Epoch: 55 | Loss: 0.24179794266819954\n",
            "Training Epoch: 55 | Loss: 0.2545547525739611\n",
            "Training Epoch: 55 | Loss: 0.2550890201526643\n",
            "Training Epoch: 55 | Loss: 0.25409069985918625\n",
            "Training Epoch: 55 | Loss: 0.25415256148450394\n",
            "Training Epoch: 55 | Loss: 0.2555365698177836\n",
            "Training Epoch: 55 | Loss: 0.2545494299719789\n",
            "Training Epoch: 55 | Loss: 0.2542830673797048\n",
            "Training Epoch: 55 | Loss: 0.25279290911238106\n",
            "Validation Loss: 0.2582538053393364\n",
            "Validation Loss: 0.3117231035070254\n",
            "Validation Loss: 0.3157672997360206\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 56 | Loss: 0.34451858699321747\n",
            "Training Epoch: 56 | Loss: 0.25066412438518637\n",
            "Training Epoch: 56 | Loss: 0.24809784395388554\n",
            "Training Epoch: 56 | Loss: 0.2499469010765816\n",
            "Training Epoch: 56 | Loss: 0.2506102842448321\n",
            "Training Epoch: 56 | Loss: 0.2508642647783467\n",
            "Training Epoch: 56 | Loss: 0.2514711786665878\n",
            "Training Epoch: 56 | Loss: 0.2515156291791911\n",
            "Training Epoch: 56 | Loss: 0.2520389233630955\n",
            "Validation Loss: 0.3856484889984131\n",
            "Validation Loss: 0.304613751423831\n",
            "Validation Loss: 0.313498935277634\n",
            "\n",
            "Training Epoch: 57 | Loss: 0.24556320905685425\n",
            "Training Epoch: 57 | Loss: 0.254963417306985\n",
            "Training Epoch: 57 | Loss: 0.2542186766974072\n",
            "Training Epoch: 57 | Loss: 0.25290219687494725\n",
            "Training Epoch: 57 | Loss: 0.2537771245868798\n",
            "Training Epoch: 57 | Loss: 0.2552692382605728\n",
            "Training Epoch: 57 | Loss: 0.25425974512395266\n",
            "Training Epoch: 57 | Loss: 0.25389930193236737\n",
            "Training Epoch: 57 | Loss: 0.2541374820081073\n",
            "Validation Loss: 0.3138052076101303\n",
            "Validation Loss: 0.3168535610236744\n",
            "Validation Loss: 0.31304097400783604\n",
            "\n",
            "Training Epoch: 58 | Loss: 0.20982881635427475\n",
            "Training Epoch: 58 | Loss: 0.2546761411583365\n",
            "Training Epoch: 58 | Loss: 0.24916794589391691\n",
            "Training Epoch: 58 | Loss: 0.24783086479081465\n",
            "Training Epoch: 58 | Loss: 0.24953352010105168\n",
            "Training Epoch: 58 | Loss: 0.2501087322823718\n",
            "Training Epoch: 58 | Loss: 0.2500267048488788\n",
            "Training Epoch: 58 | Loss: 0.25132237017133435\n",
            "Training Epoch: 58 | Loss: 0.25269418054454484\n",
            "Validation Loss: 0.3897954821586609\n",
            "Validation Loss: 0.310779708686589\n",
            "Validation Loss: 0.314363623918289\n",
            "\n",
            "Training Epoch: 59 | Loss: 0.19119109213352203\n",
            "Training Epoch: 59 | Loss: 0.255118136211197\n",
            "Training Epoch: 59 | Loss: 0.2536765395835116\n",
            "Training Epoch: 59 | Loss: 0.2542022101556344\n",
            "Training Epoch: 59 | Loss: 0.25467054202613837\n",
            "Training Epoch: 59 | Loss: 0.2540533447381622\n",
            "Training Epoch: 59 | Loss: 0.2544401509783381\n",
            "Training Epoch: 59 | Loss: 0.25369285345715226\n",
            "Training Epoch: 59 | Loss: 0.2537154835973228\n",
            "Validation Loss: 0.24486171454191208\n",
            "Validation Loss: 0.30947605061280253\n",
            "Validation Loss: 0.31376069501524245\n",
            "\n",
            "Training Epoch: 60 | Loss: 0.22287482023239136\n",
            "Training Epoch: 60 | Loss: 0.2552924432081751\n",
            "Training Epoch: 60 | Loss: 0.2493718491394573\n",
            "Training Epoch: 60 | Loss: 0.24926229786016418\n",
            "Training Epoch: 60 | Loss: 0.24885091809393314\n",
            "Training Epoch: 60 | Loss: 0.2504362029556981\n",
            "Training Epoch: 60 | Loss: 0.251349302033542\n",
            "Training Epoch: 60 | Loss: 0.2524786185308462\n",
            "Training Epoch: 60 | Loss: 0.252593388155419\n",
            "Validation Loss: 0.1946773324161768\n",
            "Validation Loss: 0.31597619769301744\n",
            "Validation Loss: 0.31389957314255224\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 61 | Loss: 0.23030699416995049\n",
            "Training Epoch: 61 | Loss: 0.2496536997986017\n",
            "Training Epoch: 61 | Loss: 0.25349570403051613\n",
            "Training Epoch: 61 | Loss: 0.2541176155983065\n",
            "Training Epoch: 61 | Loss: 0.2492897929763073\n",
            "Training Epoch: 61 | Loss: 0.24928844019123717\n",
            "Training Epoch: 61 | Loss: 0.2520117416881806\n",
            "Training Epoch: 61 | Loss: 0.25225264879977183\n",
            "Training Epoch: 61 | Loss: 0.2525301289043567\n",
            "Validation Loss: 0.28146015107631683\n",
            "Validation Loss: 0.30886189497564687\n",
            "Validation Loss: 0.3129931101714497\n",
            "\n",
            "Training Epoch: 62 | Loss: 0.3041074424982071\n",
            "Training Epoch: 62 | Loss: 0.246675212858337\n",
            "Training Epoch: 62 | Loss: 0.24878804085414802\n",
            "Training Epoch: 62 | Loss: 0.2489426415537382\n",
            "Training Epoch: 62 | Loss: 0.2516251136630105\n",
            "Training Epoch: 62 | Loss: 0.25281991478345706\n",
            "Training Epoch: 62 | Loss: 0.25225622774707696\n",
            "Training Epoch: 62 | Loss: 0.2515009253016127\n",
            "Training Epoch: 62 | Loss: 0.252921054748988\n",
            "Validation Loss: 0.3339709937572479\n",
            "Validation Loss: 0.31572378312449645\n",
            "Validation Loss: 0.3145061838741771\n",
            "\n",
            "Training Epoch: 63 | Loss: 0.2883102037012577\n",
            "Training Epoch: 63 | Loss: 0.25178731918925107\n",
            "Training Epoch: 63 | Loss: 0.252787070506396\n",
            "Training Epoch: 63 | Loss: 0.25296064555966774\n",
            "Training Epoch: 63 | Loss: 0.25193313505509846\n",
            "Training Epoch: 63 | Loss: 0.25259086510497414\n",
            "Training Epoch: 63 | Loss: 0.25286647494876147\n",
            "Training Epoch: 63 | Loss: 0.2514184988004165\n",
            "Training Epoch: 63 | Loss: 0.2519734272321735\n",
            "Validation Loss: 0.26403115689754486\n",
            "Validation Loss: 0.3141526610839485\n",
            "Validation Loss: 0.3152325005358576\n",
            "\n",
            "Training Epoch: 64 | Loss: 0.29455433040857315\n",
            "Training Epoch: 64 | Loss: 0.24494978023858943\n",
            "Training Epoch: 64 | Loss: 0.2479646620410147\n",
            "Training Epoch: 64 | Loss: 0.24878647409849786\n",
            "Training Epoch: 64 | Loss: 0.25247831735063225\n",
            "Training Epoch: 64 | Loss: 0.25102423111017713\n",
            "Training Epoch: 64 | Loss: 0.2515021426674481\n",
            "Training Epoch: 64 | Loss: 0.2517815569917885\n",
            "Training Epoch: 64 | Loss: 0.25180027454500825\n",
            "Validation Loss: 0.28314683958888054\n",
            "Validation Loss: 0.32462142407894135\n",
            "Validation Loss: 0.3157300858003138\n",
            "\n",
            "Training Epoch: 65 | Loss: 0.20447960495948792\n",
            "Training Epoch: 65 | Loss: 0.26192865485675854\n",
            "Training Epoch: 65 | Loss: 0.25748742581229306\n",
            "Training Epoch: 65 | Loss: 0.2548284253780034\n",
            "Training Epoch: 65 | Loss: 0.2530965869526316\n",
            "Training Epoch: 65 | Loss: 0.2528652942696672\n",
            "Training Epoch: 65 | Loss: 0.252085154392631\n",
            "Training Epoch: 65 | Loss: 0.2513387990469769\n",
            "Training Epoch: 65 | Loss: 0.2514045521612285\n",
            "Validation Loss: 0.2893616743385792\n",
            "Validation Loss: 0.3125750697079566\n",
            "Validation Loss: 0.3117735391918255\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 66 | Loss: 0.15510821156203747\n",
            "Training Epoch: 66 | Loss: 0.24076220324945333\n",
            "Training Epoch: 66 | Loss: 0.2427994291210056\n",
            "Training Epoch: 66 | Loss: 0.24407607141226234\n",
            "Training Epoch: 66 | Loss: 0.24624225796848312\n",
            "Training Epoch: 66 | Loss: 0.2495620723538651\n",
            "Training Epoch: 66 | Loss: 0.25168453637330784\n",
            "Training Epoch: 66 | Loss: 0.25182351184618285\n",
            "Training Epoch: 66 | Loss: 0.25172659878231834\n",
            "Validation Loss: 0.2657654210925102\n",
            "Validation Loss: 0.3128445904591296\n",
            "Validation Loss: 0.313970616810135\n",
            "\n",
            "Training Epoch: 67 | Loss: 0.16587365232408047\n",
            "Training Epoch: 67 | Loss: 0.2553880759653184\n",
            "Training Epoch: 67 | Loss: 0.25712333061727716\n",
            "Training Epoch: 67 | Loss: 0.2562284157224668\n",
            "Training Epoch: 67 | Loss: 0.254508326447887\n",
            "Training Epoch: 67 | Loss: 0.2533343856310297\n",
            "Training Epoch: 67 | Loss: 0.25255013031485224\n",
            "Training Epoch: 67 | Loss: 0.2524194890479112\n",
            "Training Epoch: 67 | Loss: 0.252025260887668\n",
            "Validation Loss: 0.23725751787424088\n",
            "Validation Loss: 0.3073451967153809\n",
            "Validation Loss: 0.31457286001882745\n",
            "\n",
            "Training Epoch: 68 | Loss: 0.22222531586885452\n",
            "Training Epoch: 68 | Loss: 0.2419650017809455\n",
            "Training Epoch: 68 | Loss: 0.2505542706346037\n",
            "Training Epoch: 68 | Loss: 0.25265837708283895\n",
            "Training Epoch: 68 | Loss: 0.25218303617730997\n",
            "Training Epoch: 68 | Loss: 0.25057582807784784\n",
            "Training Epoch: 68 | Loss: 0.25211411448328847\n",
            "Training Epoch: 68 | Loss: 0.25124033998044343\n",
            "Training Epoch: 68 | Loss: 0.25067052011204866\n",
            "Validation Loss: 0.21244322508573532\n",
            "Validation Loss: 0.3173553484058616\n",
            "Validation Loss: 0.3124331798663928\n",
            "\n",
            "Training Epoch: 69 | Loss: 0.22858159244060516\n",
            "Training Epoch: 69 | Loss: 0.24061675082043846\n",
            "Training Epoch: 69 | Loss: 0.2466422389933274\n",
            "Training Epoch: 69 | Loss: 0.2507694827522649\n",
            "Training Epoch: 69 | Loss: 0.2512408210582418\n",
            "Training Epoch: 69 | Loss: 0.2497019434065192\n",
            "Training Epoch: 69 | Loss: 0.25187678894991933\n",
            "Training Epoch: 69 | Loss: 0.2517298886488682\n",
            "Training Epoch: 69 | Loss: 0.2516749186125048\n",
            "Validation Loss: 0.41884030401706696\n",
            "Validation Loss: 0.3151773244769561\n",
            "Validation Loss: 0.3119932405039001\n",
            "\n",
            "Training Epoch: 70 | Loss: 0.2906559035181999\n",
            "Training Epoch: 70 | Loss: 0.24988937776277562\n",
            "Training Epoch: 70 | Loss: 0.24557567978120265\n",
            "Training Epoch: 70 | Loss: 0.24784566911921352\n",
            "Training Epoch: 70 | Loss: 0.250961722295146\n",
            "Training Epoch: 70 | Loss: 0.25243858027898386\n",
            "Training Epoch: 70 | Loss: 0.25233123421098747\n",
            "Training Epoch: 70 | Loss: 0.2530686267666232\n",
            "Training Epoch: 70 | Loss: 0.2533216758139348\n",
            "Validation Loss: 0.23326481878757477\n",
            "Validation Loss: 0.31579703490922945\n",
            "Validation Loss: 0.31397815777072263\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 71 | Loss: 0.25058984011411667\n",
            "Training Epoch: 71 | Loss: 0.24929917459883313\n",
            "Training Epoch: 71 | Loss: 0.25019637910436043\n",
            "Training Epoch: 71 | Loss: 0.2515296596117887\n",
            "Training Epoch: 71 | Loss: 0.251726608918492\n",
            "Training Epoch: 71 | Loss: 0.252069666626642\n",
            "Training Epoch: 71 | Loss: 0.2518666655541136\n",
            "Training Epoch: 71 | Loss: 0.2516712936241918\n",
            "Training Epoch: 71 | Loss: 0.2516671546936017\n",
            "Validation Loss: 0.4351712465286255\n",
            "Validation Loss: 0.317135948208299\n",
            "Validation Loss: 0.3152417601433708\n",
            "\n",
            "Training Epoch: 72 | Loss: 0.20518574863672256\n",
            "Training Epoch: 72 | Loss: 0.25080553089996965\n",
            "Training Epoch: 72 | Loss: 0.2521900698283122\n",
            "Training Epoch: 72 | Loss: 0.2550248701768955\n",
            "Training Epoch: 72 | Loss: 0.25390083098928085\n",
            "Training Epoch: 72 | Loss: 0.25349632388133253\n",
            "Training Epoch: 72 | Loss: 0.25346266967050746\n",
            "Training Epoch: 72 | Loss: 0.2521890454517728\n",
            "Training Epoch: 72 | Loss: 0.2523980616241898\n",
            "Validation Loss: 0.37838637083768845\n",
            "Validation Loss: 0.31080359214972153\n",
            "Validation Loss: 0.31309338865699754\n",
            "\n",
            "Training Epoch: 73 | Loss: 0.2882465571165085\n",
            "Training Epoch: 73 | Loss: 0.24496815230070365\n",
            "Training Epoch: 73 | Loss: 0.24705226636904093\n",
            "Training Epoch: 73 | Loss: 0.24864129363822185\n",
            "Training Epoch: 73 | Loss: 0.250032626844924\n",
            "Training Epoch: 73 | Loss: 0.25208512985866943\n",
            "Training Epoch: 73 | Loss: 0.2518481245038166\n",
            "Training Epoch: 73 | Loss: 0.25173900092017976\n",
            "Training Epoch: 73 | Loss: 0.2509244395008359\n",
            "Validation Loss: 0.42577845603227615\n",
            "Validation Loss: 0.3100926884182609\n",
            "Validation Loss: 0.31387564655745503\n",
            "\n",
            "Training Epoch: 74 | Loss: 0.15528523921966553\n",
            "Training Epoch: 74 | Loss: 0.25251478266597976\n",
            "Training Epoch: 74 | Loss: 0.25417905771613714\n",
            "Training Epoch: 74 | Loss: 0.2512593001832697\n",
            "Training Epoch: 74 | Loss: 0.2503965949672193\n",
            "Training Epoch: 74 | Loss: 0.2508177368449951\n",
            "Training Epoch: 74 | Loss: 0.2491982417047867\n",
            "Training Epoch: 74 | Loss: 0.25038700599420005\n",
            "Training Epoch: 74 | Loss: 0.2523268283590135\n",
            "Validation Loss: 0.25376132130622864\n",
            "Validation Loss: 0.3031175169024137\n",
            "Validation Loss: 0.31401766382565544\n",
            "\n",
            "Training Epoch: 75 | Loss: 0.26682595163583755\n",
            "Training Epoch: 75 | Loss: 0.25096405140760514\n",
            "Training Epoch: 75 | Loss: 0.2553515051635195\n",
            "Training Epoch: 75 | Loss: 0.25414198511388414\n",
            "Training Epoch: 75 | Loss: 0.2526450021206366\n",
            "Training Epoch: 75 | Loss: 0.2515096657476323\n",
            "Training Epoch: 75 | Loss: 0.2518991274439951\n",
            "Training Epoch: 75 | Loss: 0.25266801191496524\n",
            "Training Epoch: 75 | Loss: 0.2530711725256528\n",
            "Validation Loss: 0.35367968678474426\n",
            "Validation Loss: 0.3129488705740412\n",
            "Validation Loss: 0.31338237389106655\n",
            "\n",
            "==> Save checkpoint ...\n",
            "Training Epoch: 76 | Loss: 0.2608019523322582\n",
            "Training Epoch: 76 | Loss: 0.25100165961476245\n",
            "Training Epoch: 76 | Loss: 0.24950938841411427\n",
            "Training Epoch: 76 | Loss: 0.2519747130263386\n",
            "Training Epoch: 76 | Loss: 0.2502925434751329\n",
            "Training Epoch: 76 | Loss: 0.2515378263212012\n",
            "Training Epoch: 76 | Loss: 0.24986724661176593\n",
            "Training Epoch: 76 | Loss: 0.2503623492755368\n",
            "Training Epoch: 76 | Loss: 0.2511004429175687\n",
            "Validation Loss: 0.36685675382614136\n",
            "Validation Loss: 0.3150331118872555\n",
            "Validation Loss: 0.312007150015057\n",
            "\n",
            "Training Epoch: 77 | Loss: 0.31402192264795303\n",
            "Training Epoch: 77 | Loss: 0.2539995545244748\n",
            "Training Epoch: 77 | Loss: 0.2536421740770488\n",
            "Training Epoch: 77 | Loss: 0.25345115855558964\n",
            "Training Epoch: 77 | Loss: 0.25057207633673845\n",
            "Training Epoch: 77 | Loss: 0.2508290603481783\n",
            "Training Epoch: 77 | Loss: 0.2512001617596223\n",
            "Training Epoch: 77 | Loss: 0.25145987607804415\n",
            "Training Epoch: 77 | Loss: 0.2513965264697572\n",
            "Validation Loss: 0.35259535908699036\n",
            "Validation Loss: 0.318470857004718\n",
            "Validation Loss: 0.3157608229944955\n",
            "\n",
            "Training Epoch: 78 | Loss: 0.2771763876080513\n",
            "Training Epoch: 78 | Loss: 0.25649871193979046\n",
            "Training Epoch: 78 | Loss: 0.2527680734811879\n",
            "Training Epoch: 78 | Loss: 0.25266153490993865\n",
            "Training Epoch: 78 | Loss: 0.25300967030487304\n",
            "Training Epoch: 78 | Loss: 0.2536927879399466\n",
            "Training Epoch: 78 | Loss: 0.25282354858981293\n",
            "Training Epoch: 78 | Loss: 0.2522920735881859\n",
            "Training Epoch: 78 | Loss: 0.2517076882902305\n",
            "Validation Loss: 0.26424790173768997\n",
            "Validation Loss: 0.30446594197413707\n",
            "Validation Loss: 0.3137691360041128\n",
            "\n",
            "Training Epoch: 79 | Loss: 0.22120244428515434\n",
            "Training Epoch: 79 | Loss: 0.2513196947846082\n",
            "Training Epoch: 79 | Loss: 0.25600578476540486\n",
            "Training Epoch: 79 | Loss: 0.2551717949888833\n",
            "Training Epoch: 79 | Loss: 0.2555693571602094\n",
            "Training Epoch: 79 | Loss: 0.25512704943602316\n",
            "Training Epoch: 79 | Loss: 0.25365607459686956\n",
            "Training Epoch: 79 | Loss: 0.25196170484575414\n",
            "Training Epoch: 79 | Loss: 0.25128801923231237\n",
            "Validation Loss: 0.26739712804555893\n",
            "Validation Loss: 0.30745680597011404\n",
            "Validation Loss: 0.3117792162098991\n",
            "\n",
            "==> Save checkpoint ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShybkOIOlEDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}